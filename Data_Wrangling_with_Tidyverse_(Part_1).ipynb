{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Wrangling with Tidyverse (Part 1).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "dJVBVErK9i3M",
        "C5qEHrL025qw",
        "CaRtQ8V48jjT",
        "eXQ0GOwt-mOr",
        "fGYQ4slpBBeE",
        "4FUYIjPLC3W6",
        "x1itAT2yH6Be",
        "AuSTl6aJWSRa",
        "wUNAe5603S8W",
        "vr5oGII8ZxDf"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stevanbarry/NEWS2/blob/master/Data_Wrangling_with_Tidyverse_(Part_1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDMDJ6kaGE1o",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "# Data wrangling with the tidyverse\n",
        "### Exercise book (part 1)\n",
        "#### Written by Ewan Wakeman ([ewan.wakeman@nhs.net](mailto:ewan.wakeman@nhs.net))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKH9iUHg7h2k",
        "colab_type": "text"
      },
      "source": [
        "## Installing & Loading Packages\n",
        "\n",
        "Base R is very limited in terms of the functions availible to do common data preparation, modelling, and visualisatoin tasks. But thankfully there are lots of wonderful packages (collections of methods, classes and functions) built in R which you can install, load, and use in your code.\n",
        "\n",
        "Most R packages you will use are availible through CRAN (Comprehensive R Archive Network) and can be installed using the function `install.packages()`.\n",
        "\n",
        "The _tidyverse_ is actually a collection of a number of different packages for accessing, tidying, plotting and modelling data. All of these packages can be installed at once by installing tidyverse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTStFCsn9v6l",
        "colab_type": "text"
      },
      "source": [
        "### Installing Packages\n",
        "\n",
        "Install the tidyverse by completing the code below on line 4.\n",
        "\n",
        "##### Further Questions\n",
        "  1. What do you think the code in line 1 and 2 will do.\n",
        "  2. What do you think the purpose of the function `c()` is in line 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCOItNkXGXoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# package_list <- c('tibble', 'dplyr', 'tidyr', 'readr', 'ggplot2', 'purrr')\n",
        "# install.packages(list = package_list)\n",
        "\n",
        "install.packages()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJVBVErK9i3M",
        "colab_type": "text"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vMyncoG9pjT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "install.packages('tidyverse')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSOHydNC2GJi",
        "colab_type": "text"
      },
      "source": [
        "### Loading Packages\n",
        "\n",
        "Load the tidyverse packages by completing to the information on line 5. Create a message to print if the package loads successfully."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7CE1ib2HWmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading packages\n",
        "#library('package_name') # this is the usual way of loading packages\n",
        "#require('package_name') # this works too but returns \"TRUE\" if a package has loaded\n",
        "\n",
        "if(require(______)) print(_______)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5qEHrL025qw",
        "colab_type": "text"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83Kz7OCv2-yw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if(require('tidyverse')) print('tidyverse loaded successfully')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOWmIc4EIT4D",
        "colab_type": "text"
      },
      "source": [
        "## Fetching data\n",
        "\n",
        "We're going to grab some of the latest Covid-19 data from the government website - the latest:\n",
        "  - Cases\n",
        "  - Deaths\n",
        "\n",
        "To help us uderstnad in more detail we'll combine with some ONS data on:\n",
        "  - Population\n",
        "  - Deprivation\n",
        "\n",
        "We'll start with the Government Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtX4gt9pMQxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# link to government Coronavirus data\n",
        "cases_url <- 'https://coronavirus.data.gov.uk/downloads/csv/coronavirus-cases_latest.csv'\n",
        "deaths_url <- 'https://coronavirus.data.gov.uk/downloads/csv/coronavirus-deaths_latest.csv'\n",
        "\n",
        "# link to ONS data\n",
        "ltlapop_url <- \"https://www.ons.gov.uk/file?uri=%2fpeoplepopulationandcommunity%2fpopulationandmigration%2fpopulationestimates%2fdatasets%2fpopulationestimatesforukenglandandwalesscotlandandnorthernireland%2fmid2019april2020localauthoritydistrictcodes/ukmidyearestimates20192020ladcodes.xls\"\n",
        "ltlaregion_url <- \"https://opendata.arcgis.com/datasets/3ba3daf9278f47daba0f561889c3521a_0.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M307a_avQZim",
        "colab_type": "text"
      },
      "source": [
        "## Loading the data\n",
        "\n",
        "Usually the process of reading data requires downloading -> storing the file you need locally -> then loading into R. Sometimes this is desireable but with small files that you're only using as part of a data cleaning pipeline (e.g. some of the .csv files above that change regularly, we may just want to load them straight into memory. We can always save a cleaned version of the combined files a little later in the process once it looks more like the data we want.\n",
        "\n",
        "Most of the files here are .csv files which is great but one is an Excel file. The tidyverse doesn't load anything by default to deal with Excel files but thankfully there are plenty of tidyverse-adjacent packages to deal with reading excel files. We'll use \"readxl\" but first we'll need to grab and load it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UP762gI94GMH",
        "colab_type": "text"
      },
      "source": [
        "Look at the code below and consider the following:\n",
        "\n",
        "1. what do you think this code is going to do?\n",
        "2. what will the output look like?\n",
        "3. why are we using `require` instead of `library` here?\n",
        "4. what will happen if we run the code a second time after the first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5Caa3sbRofi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if(!require('readxl')){\n",
        "  install.packages('readxl')\n",
        "  require('readxl')\n",
        "} else {TRUE}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-oCpp5l4Y8R",
        "colab_type": "text"
      },
      "source": [
        "### Reading Excel Files\n",
        "\n",
        "As Excel files can contain multiple sheets with data in, we should first examine the contents of a file before trying to load it. WE can do this in Excel if we need to, but we should be able to do that entirely in R.\n",
        "\n",
        "In the below cell do the following:\n",
        "1. Create a temporary file with an extension of '.xls' using the `tempfile()` function and using the `fileext` argument to define the extension. store the result of this function in a variable (e.g. t)\n",
        "  - if you `print()` the variable just created, what do you think it will show you?\n",
        "2. Download a copy of the file using the `download.file()` function using the variable `ltlapop_url` we created earlier as the first argument (`url`) and passing the `destfile` argument the temporary file we created in step 1. Also add the arguement `mode = 'wb'` to the function to specify we want `download.file` to write a _binary_ version of the file (this is required for excel files.\n",
        "3. Use the `excel_sheets()` function on the file we just donwnloaded to view the shets in the excel file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFKtFRgfSqnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tempfile()\n",
        "\n",
        "download.file()\n",
        "\n",
        "excel_sheets()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaRtQ8V48jjT",
        "colab_type": "text"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frBPAbut8liQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t <- tempfile(fileext = '.xls')\n",
        "\n",
        "download.file(ltlapop_url, destfile = t, mode = 'wb')\n",
        "\n",
        "excel_sheets(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqCSDYZm8pth",
        "colab_type": "text"
      },
      "source": [
        " ### Working with messy files\n",
        "\n",
        " Using the ouptut from the last step use the `read_excel()` function to read the appropriate sheet of the file (We're looking for the one with 'Persons' in it). The first argument will need to be the location of the file we want to read and we wil also need to specify the `sheet` argument with a text string giving the name of the sheet we want to read.\n",
        "\n",
        " After loading the sheet we'll use the `head()` function to show the table we've just loaded (first argument) and pass 15 as a value to the second argument to print the top 15 rows of the tibble."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywIq39FjTpjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pop_data <- read_excel()\n",
        "\n",
        "head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXQ0GOwt-mOr",
        "colab_type": "text"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4o6uk9t-oNy",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "pop_data <- read_excel(t, sheet = 'MYE2 - Persons')\n",
        "head(pop_data, 15)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvHF2iga_mwr",
        "colab_type": "text"
      },
      "source": [
        "#### Dealing with _structured_ files\n",
        "\n",
        "Looking at the output of this data - we've got a bit of an issue. The first row of the file doesn't contain the colum headings for the table and the first couple of rows contain headings or blanks, rather than data relating to what we're interested in.\n",
        "\n",
        "This is common for Excel files where data is viewed as a spreadsheet where each sheet may conatin multipe data tables, headings and notes as well as charts or images. This file isn't particularly complicated as it simply has one table of data, but this is positioned beneath some headings and other explanatory wording.\n",
        "\n",
        "Luckily we can use the `skip` argument in the `read_excel` function to specify a number of rows that we want the function to ___skip___ before reading the data. Let's see if we can get the data loading nicely by rewriting our function from before but this time specifying a number to the skip function. Use the below cell and modify the number passed to the skip function, until our output shows the column names `[Code, Name, Geography1, etc...]` when we use the head function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vx-ejE0yUG5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pop_data <- read_excel()\n",
        "\n",
        "head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGYQ4slpBBeE",
        "colab_type": "text"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeTxiKQCBDxy",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "pop_data <- read_excel(t, sheet = 'MYE2 - Persons', skip = 4)\n",
        "\n",
        "head(pop_data, 5)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCmcw5NXCSAl",
        "colab_type": "text"
      },
      "source": [
        "#### Some further checks\n",
        "\n",
        "Now we have the top of the file looking how it's supposed to lets  just have a little look at the bottom too. Whilst our table may now have read properly, there may still e some stuff dropped under the table that we need to know about before strating to look at the data in more detail.\n",
        "\n",
        "You can use the `tail()` function in the cell below to inspect the bottom of the data table. This function is used in the same way as `head`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x4l2xUcC0-2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FUYIjPLC3W6",
        "colab_type": "text"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ1i6zItC5EM",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "tail(pop_data)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odob8q_CU83B",
        "colab_type": "text"
      },
      "source": [
        "## Filtering data (and introducing the pipe `%>%`)\n",
        "\n",
        "The main way of filtering data in the tidyverse is using `filter()` from the dplyr package. `filter()` allows us to pass a series of expressions that evaluate to a _boolean vector_ _(aka 'logical')_ (TRUE/FALSE) and will return any rows where the true argument is matched.\n",
        "\n",
        "The pipe `%>%` allow us to string functions together in a chain passing the result of the function before the pipe to the first argument of the function after it. It is one of the fundemental building blocks of data wrangling in the tidyverse and leads to more readable code.\n",
        "\n",
        "Let's try to filter this _tibble_ to only include local authority data for england and to strip out regions, national summaries and data for scotland and wales which we don't have (at local authority level) in our gov't coronavirus data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHozAPPWDrvd",
        "colab_type": "text"
      },
      "source": [
        "### Grouping and Summarising Data\n",
        "\n",
        "\n",
        "The Population data we've loaded contains population estimates for a number of different Geographies within the UK. The \"Geography1\" column tells us what type of geography each \"Code\" (column 1) and \"Name\" (column 2) relates to.\n",
        "\n",
        "To help us get a sense of what's going on with this data we can use the `group_by()` to group all of the data by some columns in our data and then the `summarise` function to create summaries of the data within each group. `group_by` will group together rows in our data that have the same value(s) in the column(s) we specify ensuring that any summaries we create afterwards using `summarise()` will calculate for each group.\n",
        "\n",
        "Let's create a series of functions chained together by pipes (`%>%`) that take us from `pop_data` through a `group_by` function specifying `Geography1` as the grouping column and then `summarise()` with a named argument `'row_count' = n()` to count the number of rows.\n",
        "\n",
        "One important thing to note about the tidyverse is that tidyverse functions always take `data` as a first argument (usually a tibble) allowing for them to be used with the pipe `%>%` more effectively. Because of this column names that are provided after the `data` argument don't need to be provided as text strings and can just be written e.g. ` data %>% group_by(column_name)` as opposed to `data %>% group_by('column_name')`. This is different from most programming languages and other packages written in R. This is known as _quasiquotation_ in tidyverse speak."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MLeCsPSUQmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pop_data %>% \n",
        "  group_by() %>% \n",
        "  summarise()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1itAT2yH6Be",
        "colab_type": "text"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh_zUtdPH8H1",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "pop_data %>%\n",
        "  group_by(Geography1) %>%\n",
        "  summarise('row_count' = n())\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKWTU-aLJRCG",
        "colab_type": "text"
      },
      "source": [
        "### Extending `summarise()`\n",
        "\n",
        "Multiple arguments can be passed to summarise, each telling us something different about our data. In the code above we only created one column `row_count` to show how many data points there were in each group but we can use all kinds of different functions to summarise data.\n",
        "\n",
        "Create a summarise function with a column and each of the following functions<sup>*</sup>:\n",
        "1. ```mean(`All ages`, na.rm = T)```\n",
        "2. ```max(`All ages`, na.rm = T)```\n",
        "3. ```min(`All ages`, na.rm = T)```\n",
        "4. `list(Code[1:5])`\n",
        "5. `list(Name[1:5])`\n",
        "\n",
        "<sup>*</sup>__hint__: you can name a each column produced by the `summarise` function by using the syntax `'column_name' = function()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pb2mv4K-V2GD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pop_data %>%\n",
        "  group_by(Geography1) %>%\n",
        "  summarise(\n",
        "    'row_count' = n(),\n",
        "    ...\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuSTl6aJWSRa",
        "colab_type": "text"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY7AcWLGWVls",
        "colab_type": "text"
      },
      "source": [
        "```{r}\n",
        "pop_data %>%\n",
        "  group_by(Geography1) %>%\n",
        "  summarise(\n",
        "    'row_count' = n(),\n",
        "    'mean_pop' = mean(`All ages`, na.rm = T),\n",
        "    'min_pop' = min(`All ages`, na.rm = T),\n",
        "    'max_pop' = max(`All ages`, na.rm = T),\n",
        "    'top5_codes' = list(Code[1:5]),\n",
        "    'top5_names' = list(Name[1:5])\n",
        "  )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8seoCOlGX85v",
        "colab_type": "text"
      },
      "source": [
        "### Filtering data\n",
        "\n",
        "From looking at our summarise function we can see that we don't need all the area types from this table. In fact the only ones we want are those with codes beginning in 'E0' (Metropolitan Distrcits, Non-Metropolitan Districts, Unitary Authorities, and London Boroughs). We can use the `filter()` function to select only these rows easily. There are some different approches to doing this.\n",
        "\n",
        "1. We could filter to the geographies we want using a series of statements separated by an or\n",
        "```\n",
        "pop_data_filtered <-\n",
        "    pop_data %>%\n",
        "    filter(Geography1 == 'County' | Geography1 == 'London Borough' | Geography1 == etc...)\n",
        "```\n",
        "\n",
        "2. We could provide a list of valid geographies and check whether our value is _\"in\"_ the list.\n",
        "```\n",
        "# define geographies we want\n",
        "geo_keep <- c('London Borough', 'Metropolitan District', 'Non-metropolitan District', 'Unitary Authority')\n",
        "# filter\n",
        "pop_data_filtered <- \n",
        "    pop_data %>% \n",
        "    filter(Geography1 %in% geo_keep)\n",
        "```\n",
        "\n",
        "3. Another option - and the one we'll try, is to use __regular expressions__ (regex) to filter based on the values in the `Code` column. We know that all the geographies we are interested in start with _'E0'_ and so we can use this as a _pattern_ with which to filter. The functions `str_starts(string, pattern, negate = False)` and `str_detect(string, pattern, negate = False)` are two good places to start. `str_starts` will only check whether a string starts with the pattern, returning True if it does and False if it doesn't, wheras `str_detect` will look for the pattern anywhere in the string (still returns True/False).\n",
        "\n",
        "Using our pattern below, let's try to filter the table using the code column and `str_starts()` and then use `group_by() %>% summarise()` as we did above to examine the result. If you want you can try rerunning the function with `str_detect` as well to see if it makes any difference in our particular example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrgwhicNb51G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pop_data_filtered <-\n",
        "  pop_data %>%\n",
        "  filter(str_starts())\n",
        "\n",
        "pop_data_filtered %>%\n",
        "  group_by() %>%\n",
        "  summarise()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCg9uAgebBH0",
        "colab_type": "text"
      },
      "source": [
        "##### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NqplZFgc5ZX",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "pop_data_filtered <- \n",
        "  pop_data %>% \n",
        "  filter(str_starts(Code, 'E0'))\n",
        "\n",
        "pop_data_filtered %>%\n",
        "  group_by(Geography1) %>%\n",
        "  summarise(\n",
        "    'row_count' = n(),\n",
        "    'mean_pop' = mean(`All ages`, na.rm = T),\n",
        "    'min_pop' = min(`All ages`, na.rm = T),\n",
        "    'max_pop' = max(`All ages`, na.rm = T),\n",
        "    'top5_codes' = list(Code[1:5]),\n",
        "    'top5_names' = list(Name[1:5])\n",
        "  )\n",
        "  ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ9oGAPacHf3",
        "colab_type": "text"
      },
      "source": [
        "### Renaming and some last bits of cleaning\n",
        "\n",
        "Finally, before we finish up with this file, let's rename the columns to get rid of those pesky spaces in the column names that meant we had to keep using backticks when referencing them. To do this we could go through and rename each column in turn using the `rename()` function. That would look a little something like this.\n",
        "\n",
        "```\n",
        "pop_data_renamed <-\n",
        "  pop_data_filtered %>%\n",
        "  rename('code' = Code, 'name' = Name, 'geo' = Geography1, 'total_pop' = `All Ages`)\n",
        "```\n",
        "This is pretty straightforward to write and read, but could become a little tiring if we have lots of columns to work with. So let's try and define some rules to clean up our column names. To achieve the same result as above we really want to:\n",
        "  1. Strip the 'graphy1' off the end of Geography1 to make 'Geo'\n",
        "  2. Replace any spaces with underscores\n",
        "  3. convert things from upper to lowercase\n",
        "\n",
        "As a belt and braces approach we can also make sure none of our columns have anything weird going on like spaces at the end of column names or double spaces between words using a function `str_squish()` which ensures things don't have any leading, trailing or double spaces.\n",
        "\n",
        "We can define this set of rules as a pipeline using our pipe `%>%` and pass all of it to a function called `rename_all()`.\n",
        "\n",
        "Stitching it together gives us a full function that looks like this:\n",
        "```\n",
        "rename_all(~str_squish(.x) %>% str_replace_all(c('\\\\s' = '_', 'graphy1' = '')) %>% str_to_lower)\n",
        "```\n",
        "There are a few things going on here that it's important to know about.\n",
        "  1. We're using the tilde `~` symbol to tell `rename_all` that we're giving it a function. Normally in R you define a function by writing `function(args){ code in here }` but when we're writing functions in tidyverse functions we can use `~` as a shorthand to say everything after this is a _function_ which should be applied to each element (in this case each column) because `rename_all` is concerned with renaming columns.\n",
        "  2. Because we're using functions, we also need to point out where we want our column name (which is the thing `rename_all` is going to pass onto the function) to come in our function. We tell it where to put the column name in the sequence using `.x` which is tidyverse speak for the _first argument_. In practice you can think of `.x` representing each column name in turn when we run `rename_all`. So at first `.x` will represent 'Code', then 'Name', then 'Geography1' and so on. Each column name will therefore start of being _squished_ by `str_squish` before being passed onto a `str_replace_all` (to replace some bits of text with others) and lastly `str_to_lower` (to drop everything to lowercase).\n",
        "  3. `str_replace_all()`'s purpose, should hopefully be pretty straightforward (it replaces something with something else), though it's implementation is a little odd here. We're actually using the output of the `str_squish` as the first argument (passed on from the pipe `%>%` then giving it a _vector of replacements_ using `c()`. Each element of the vector is a replacement rule and looks like 'thing to look for' = 'thing to replace with'. We have two of thise, which I'll go into in more detail below:\n",
        "    \n",
        "      a. `\\\\s = '_'` - `\\\\s` is regex speak for a space. The first `\\` tells the function to interpret the next bit as a special characters and the second `\\s` represents a space. This is a little hard to wrap your head around at first but it makes sense when you understand we can also represent returns /newlines as `\\\\n` or other speacil characters such as tab `\\\\t` in this way. Often these characters have special meanings in languages so we want to make sure our code is clear from R's point of view. the second half of this should be a little clearer, we're replacing spaces with the `_` symbol. This is going to help us on our journey towards snake_case.\n",
        "\n",
        "      b.  `graphy1 = ''` - This time the first half is pretty straightforward, we're looking for the string 'graphy1'. We're replacing it with the text equivalent of nothing `''`. As we've used single quotation marks to declare to R that we're giving it a string it will be properly recognised as a valid string, but the fact we put nothing between them essentially removes this string entirely. We could equally have used `'Geography1' = 'Geo'` here or used `str_remove('graphy1')` instead but this is a handy trick to know about as it allows you to do both replacement and removal functions in one go.\n",
        "\n",
        "\n",
        "Now that that's all sorted, lets try using the `rename_all` function above to create a new variable `pop_data_renamed` to hold our renamed columns. \n",
        "\n",
        "After this we'll use the `select()` function to select the columns we're interested in (for now 'Code', 'Name', 'Geography1' and 'All Ages') and use that to create `pop_data_aa` as a table with the total population across `all_ages`. To test our understanding of the previous renaming function we'll do the `select()` function __after renaming__ the columns. This means we'll need to provide `select` with the column names as they will appear __after__ we run `rename_all`. Column names can be provided to select either as a vector of names `select(data, c(col1, col2, etc...))` or as multiple arguments seperated by commas `select(data, col1, col2, etc...)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS_XRmx_ejs0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pop_data_renamed <-\n",
        "  pop_data_filtered\n",
        "  rename_all()\n",
        "\n",
        "pop_data_aa <-\n",
        "  pop_data_renamed\n",
        "  select()\n",
        "\n",
        "head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUNAe5603S8W",
        "colab_type": "text"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl5eDuEz3Siw",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "pop_data_renamed <-\n",
        "  pop_data_filtered %>%\n",
        "  rename_all(~str_squish(.x) %>% str_replace_all(c('\\\\s' = '_', 'graphy1' = '')) %>% str_to_lower)\n",
        "\n",
        "pop_data_aa <-\n",
        "  pop_data_renamed %>%\n",
        "  select(code, name, geo, all_ages)\n",
        "\n",
        "head(pop_data_renamed)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvrQiQQx3f1P",
        "colab_type": "text"
      },
      "source": [
        "### Pivoting data\n",
        "\n",
        "In the last step we created a cleaned dataset and simply stripped out the columns that gave the population sizes for individual ages. However, we might late be interested in the age profile of areas as well as the total size - though it's unlikely we'll need to see each individual year.\n",
        "\n",
        "The age data is stored in 91 seperate columns each representing an age from 0 - 90+. This is an example of some unnecessarily _wide_ data. It would make more sense to have this as two columns where one gave the age and the other gave the population for that age. Then we could use the `group_by` and `summarise` functions from previous steps to quickly calculate some age bands. Thankfully pivoting data into long and wide formats is easy with the tidyverse. There are two functions `pivot_longer` (historically `gather`) and `pivot_wider` (historically `spread`) which handle these operations with ease.\n",
        "\n",
        "We can use `pivot_longer` to convert all those age columns into a longer format where each row gives us a single age and corresponding population size quickly slimming our table from a 95 column whopper to a slender 6 columns (albeit a much longer one). To do this we need to provide `pivot_longer` with a list of columns we want it to convert to rows in this way. We want 91 of the 95 columns to go into a longer format though, and writing out all the numbers from 0-90 sounds like an arduous task not to mention pretty difficult to read code.\n",
        "\n",
        "Thankfully there are (at least) two simple solutions to this problem:\n",
        "  1. Instead of specifying the columns we want to pivot, we can just exclude the ones we don't. Instead of providing a list of columns using `c()` we instead use `-c()` and instantly we reverse the specification to mean _everything except_ the columns we mention.\n",
        "  2. We can use the regex matching patterns we used earlier to specify a _pattern_ to return the columns we want. We know all of our columns containing ages are written as either one or two numbers (e.g. 5 or 29). We can write this as a regex pattern and then use `str_which` to return the column names of anything which follows this pattern. unlike `str_start` or `str_detect`, `str_which` gives us a vector of names of which match, rather than a vector of TRUE/FALSE making it perfect for our needs.\n",
        "  \n",
        "  We can write this as `str_which(colnames(data), '[0-9]{1,2}')`. There are a few things going on here.\n",
        "    \n",
        "    a. `colnames()` is a function that returns the column names for any dataframe/tibble we give it.\n",
        "    \n",
        "    b. the `[0-9]` regex pattern accepts any digit between 0 and 9 (and therefore any digit. We can simalarly use `[a-z]` for all lowercase letters, `[A-Z]` for all uppercase letters, or `[1-3]` for any digit between 1 and 3 (i.e. 1, 2, and 3).\n",
        "    \n",
        "    c. the `{1,2}` regex pattern is a _repeater_ which looks tells the searching function to look for between 1 and 2 repeating instances of whatever came before (in this case any digit). if we had ages up to 100 we could easliy modify this to `[0-9]{1,3}`.\n",
        "    \n",
        "    d. lastly `str_which` brings this all together.\n",
        "\n",
        "Let's define a variable `pivot_cols` which uses str_which and our regex above to create a list of column names in our data which have either 1 or two repeating digits. Then lets pass this to pivot longer to make unpivot our columns. We'll also provide the `names_to` and `values_to` arguments to name the columns that get creted by pivoting the data. We've also included a `names_transform` argument to clean up our new age column (take from column header names), which extracts only the numbers and then converts the data from text to integers. We could have also done this by using `mutate('age' = str_extract(age, '[0-9]*') %>% as.integer)` after the `pivot_longer` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2brT1Io3kqI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pivot_cols <- str_which(colnames(), '[]{}')\n",
        "\n",
        "pop_data_by_age <-\n",
        "  pop_data_renamed %>%\n",
        "  pivot_longer(cols = ???, names_to = 'age', values_to = 'pop',\n",
        "              names_transform = list('age' = ~str_extract(.x, '[0-9]*') %>% as.integer))\n",
        "\n",
        "head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDF1T2yjPJAU",
        "colab_type": "text"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzgtvzH7PIu9",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "pivot_cols <- str_which(colnames(pop_data_renamed), '[0-9]{1,2}')\n",
        "\n",
        "pop_data_by_age <-\n",
        "  pop_data_renamed %>%\n",
        "  pivot_longer(cols = pivot_cols, names_to = 'age', values_to = 'pop', \n",
        "              names_transform = list('age' = ~str_extract(.x, '[0-9]*') %>% as.integer))\n",
        "\n",
        "head(pop_data_by_age)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARLnz76QRI71",
        "colab_type": "text"
      },
      "source": [
        "### Grouping and Summarising Data (Round 2)\n",
        "\n",
        "Now we have our data in long form we can create a new column `age_band` to give a banding for ages, `group_by` that column (and geography code and name) and then `summarise` with a `sum` function to calculate the total population in each age band.\n",
        "\n",
        "Creating new columns in with the tidyverse is done with the `mutate` function. This takes a dataframe as its first argument (usually data passed from the pipe `%>%`) and then a series of column specifications of the format `'column_name' = column_value` (as with `summarise`).\n",
        "\n",
        "Obvioulsly a lot of the time we want the value of a column to be based on values in one (or multiple) other columns in the data and this again is simple with `mutate`. For example if we wanted the population in thousands we could simply add a column which divided the total population by 1000 (i.e. `'pop_thou' = pop / 1000`). We can use this with a neat _floor division_ trick to get age bands.\n",
        "\n",
        "Here we do four things:\n",
        "  1. Define a `bandsize` argument specifying the size of our bannds\n",
        "  2. Floor divide (`%/%`) the value in the age column by our `bandsize` and multiply it out by bandsize. This essentially rounds down to the next integer which is a multiple of our bandsize (giving us our lower bound)\n",
        "  3. Do the same operation but this time add our age band and subtract 1 to give the top end of our age band.\n",
        "  4. use `paste` to squish these bits together separated by a dash.\n",
        "\n",
        "The function for this is given below but we'll need to specify a bandsize (say 10) and add a `group_by` and `summarise` to get a summary of the population for each age band in each area."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcDNZ6osS9wS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bandsize <-\n",
        "\n",
        "pop_data_by_age_band <-\n",
        "  pop_data_by_age %>%\n",
        "  mutate(age_band =  paste((age%/%bandsize)*bandsize, ((age%/%bandsize)*bandsize)+(bandsize-1), sep = '-')) %>%\n",
        "  group_by() %>%\n",
        "  summarise(pop =)\n",
        "\n",
        "head(pop_data_by_age_band, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0BucZkiWhh-",
        "colab_type": "text"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3kwP6nCWj31",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "bandsize <- 10\n",
        "\n",
        "pop_data_by_age_band <-\n",
        "  pop_data_by_age %>%\n",
        "  mutate(age_band =  paste((age%/%bandsize)*bandsize, ((age%/%bandsize)*bandsize)+(bandsize-1), sep = '-')) %>%\n",
        "  group_by(code, name, age_band) %>%\n",
        "  summarise(pop = sum(pop, na.rm = T))\n",
        "\n",
        "head(pop_data_by_age_band, 10)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjNfa2GCmMtJ",
        "colab_type": "text"
      },
      "source": [
        "## Exploring data using ggplot2\n",
        "\n",
        "Now we have some population size data (and some info about age bands) to contextualise our case and death rates, it would be good to know what we're working with. Are Lower-Tier Local Authorities all the same size (roughly), or wildly different, could that have an impact on case rates or is it irrelevant, what about age splits, how do they look nationally. We can try to look at this using descriptive stats, but these are sometimes misleading, would be far better to plot this.\n",
        "\n",
        "The reason for this is nicely demonstrated by the \"Datasauras Dozen\"\n",
        "\n",
        "![](https://d2f99xq7vri1nk.cloudfront.net/DinoSequentialSmaller.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwuGtv0WW3g4",
        "colab_type": "text"
      },
      "source": [
        "### Histograms\n",
        "\n",
        "One of the most useful things we can do to start is to visualise the distribution of population as an age band. We can do this by using the function below. There are a few things to note:\n",
        "\n",
        "  1. We create a new plot by using `ggplot`. This is the command for creating any visualisation in ggplot.\n",
        "  2. The first argument is always our tibble which gives the initial scope of what will be plotted.\n",
        "  3. Secondly we provide a function `aes()` which tells ggplot what data (columns) we want to _map_ to each _aesthetic_. For a historgram we only need to define one aesthetic (x) which is whatever we want to plot.\n",
        "  4. We add `geom_histogram()` to the plot using `+ geom_histogram()`. Notice in the ggplot world we add layers together using `+` not the pipe.\n",
        "\n",
        "This is all we actually need in order to define a histogram as ggplot handily provides loads of sensible defaults to figure out the other bits we need such as how big to make the scales, what bin sizes to use on our x axis, and what colours, labels and other visual helpers to include. In the background lots of stuff is created which we can tweak and fine tune later by adding `+` more _layers_ or _transformations_ to the plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e_YC3ANnqXA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ggplot(pop_data_aa, aes(x = all_ages)) + geom_histogram()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHXSYg_gnVz4",
        "colab_type": "text"
      },
      "source": [
        "this looks like it could almost be a skewed normal distribution but there are some clear outliers. Let's see if we can seee what geographies they belong to with a boxplot. Modify the code below to use `geo` for our `y` axis and `all_ages` for our `x` axis. We'll also pass `geo` to the `colour` aesthetic to add a splas of colour.\n",
        "\n",
        "On top of this we add `theme_minimal` to make our chart less noisy and use `theme(legend.position = 'none')` to get rid of the legend."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8swcA05nKAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ggplot(pop_data_aa, aes(y = , x = , colour = )) + \n",
        "  geom_boxplot() + \n",
        "  theme_minimal() + \n",
        "  theme(legend.position = 'none')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr5oGII8ZxDf",
        "colab_type": "text"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t10KVH9Zz9P",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "ggplot(pop_data_aa, aes(y = geo, x = all_ages, colour = geo)) + \n",
        "  geom_boxplot() + \n",
        "  theme_minimal() + \n",
        "  theme(legend.position = 'none')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekkz_1jmrAPm",
        "colab_type": "text"
      },
      "source": [
        "### Adding detail\n",
        "\n",
        "It would be better if we could get a sense of what the outliers were so let's try setting up a label for them by adding a column with the name of the local authority if it breaks the 1.5 x inter-quartile range rule.\n",
        "\n",
        "The below code implements this rule for each `geo` group separately. An important point of note here is that using `group_by` before `mutate` means that each calculation is done group by group as opposed to across all the data in the table. When we use `quantile(all_ages, 75)` to calculate the 75th percentile in the all_ages column, it does this for each group rather than all data. Doing this allows us to create the interquartile range for each geography (e.g. metropolitan district) which is important for when we plot later.\n",
        "\n",
        "Once we've added some calculation columns we can define a label which returns the area name if the population for that area is deemed to be an _outlier_ and blank if it isn't.\n",
        "\n",
        "We then map this to the `label` aesthetic and add another geom `geom_text()` to the plot to label our outliers. This is a touch more complex than what has been outlined so far, but try to follow the code through and think about what may be produced at each stage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGbSXWZ-rqOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pop_data_aa %>%\n",
        "  group_by(geo) %>%\n",
        "  mutate(\n",
        "    upr = quantile(all_ages, .75),\n",
        "    lwr = quantile(all_ages, .25),\n",
        "    iqr = upr-lwr,\n",
        "    olab = if_else(all_ages > (upr + iqr*1.5) | all_ages < (lwr - iqr*1.5), name, '')\n",
        "  ) %>%\n",
        "  ungroup() %>%\n",
        "  ggplot(aes(y = all_ages, x = geo, colour = geo, label = olab)) +\n",
        "  geom_boxplot() +\n",
        "  geom_text(angle = 0, hjust = 0, vjust =0.5) +\n",
        "  theme_minimal() +\n",
        "  theme(\n",
        "    legend.position = 'none'\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CcvjEQztxRU",
        "colab_type": "text"
      },
      "source": [
        "From this we get a bit of picture of what's going on. Non-Metropolitan districts tend to be small with the majority of them having a population of under 150,000. Unitary Authorities, London Boroughs and Metropolitan Distrcits tend to be bigger. There are also some that are much bigger than most either being larger UK cities (e.g. Sheffield, Liverpool, Bradford, Manchester) or Counties (Cornwall, Bucks County Durham, Wiltshire). Leeds and Birmingham are huge as they represent very large UK cities that are not divided up into sub areas (e.g. London or Greater Manchester)\n",
        "\n",
        "There isn't much we can do about the differences in population other than to accept that there are differences. The fact that these differences exist and are so large, suggest it's something we probably want to control for when we come to examine rates of Covid. Clearly 500 cases in the City of London would means something different to 500 cases in Birmingham."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opgtpbmfhqnP",
        "colab_type": "text"
      },
      "source": [
        "# End of Part 1\n",
        "\n",
        "Hopefully this set of excercises helped you to get familiar with the mechanics of the tidyverse and learn how this can be used to help _wrangle_ data into shape. As this is a live code book, feel free to continue to edit this workbook, changing bits or adding more code below.\n",
        "\n",
        "In a future edition we will look at some more complex plotting configurations in ggplot; iterating over multiple inputs using `map` and `reduce`; joining together different data sources using `left_join` and `inner_join`; and working with dates and sliding (window) functions with the `lubridate` and `slider` packages (part of the wider tidyverse).\n",
        "\n",
        "If you have any more questions about this or data wrangling with the tidyverse please get in touch with Ewan Wakeman (<ewan.wakeman@nhs.net>)and I'll try to respond to any questions. I'm working on part 2 now, so bear with me (there's a lot to cover)!"
      ]
    }
  ]
}